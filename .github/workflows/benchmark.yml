name: Performance Benchmark

on:
  pull_request:
    branches: [main]
    paths:
      - 'assets/js/vr-*.js'
      - 'tools/benchmark.js'
  schedule:
    # æ¯Žé€±æ—¥æ›œæ—¥ 00:00 UTC ã«å®Ÿè¡Œ / Run every Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      iterations:
        description: 'Number of iterations'
        required: false
        default: '500'

jobs:
  benchmark:
    name: Run Performance Benchmark
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run benchmark
        run: |
          ITERATIONS=${{ github.event.inputs.iterations || '500' }}
          node tools/benchmark.js --all --iterations $ITERATIONS --output benchmark-results.md --format markdown

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results.md
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            if (!fs.existsSync('benchmark-results.md')) {
              console.log('Benchmark results file not found');
              return;
            }

            const results = fs.readFileSync('benchmark-results.md', 'utf8');

            // PR ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ•ç¨¿ / Post comment to PR
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸ“Š Performance Benchmark Results\n\n${results}\n\n---\n*Automated benchmark from GitHub Actions*`
            });

      - name: Generate JSON results
        run: |
          node tools/benchmark.js --all --iterations 500 --output benchmark-results.json --format json

      - name: Upload JSON results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-json-${{ github.sha }}
          path: benchmark-results.json
          retention-days: 30

      - name: Check performance regression
        run: |
          echo "Checking for performance regressions..."

          # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¹³å‡èª­ã¿è¾¼ã¿æ™‚é–“ã‚’æŠ½å‡º
          # Extract average load time from JSON file
          if [ -f benchmark-results.json ]; then
            # ã“ã“ã§ã¯ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿéš›ã«ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒã‚’å®Ÿè£…ï¼‰
            # Simple check here (should implement comparison with baseline)
            echo "âœ“ Benchmark data collected"

            # èª­ã¿è¾¼ã¿æ™‚é–“ãŒ20msä»¥ä¸Šã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è­¦å‘Š
            # Warn about modules with load time > 20ms
            cat benchmark-results.json | jq -r '.[] | select(.loadTime.mean > 20) | "âš ï¸  " + .module + ": " + (.loadTime.mean | tostring) + "ms"' || true
          fi

      - name: Create performance summary
        run: |
          echo "## Performance Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f benchmark-results.md ]; then
            # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ã‚µãƒžãƒªãƒ¼ã«è¿½åŠ 
            cat benchmark-results.md >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Benchmark results not available" >> $GITHUB_STEP_SUMMARY
          fi

  compare-baseline:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: benchmark

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download current benchmark
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-json-${{ github.sha }}
          path: ./current

      - name: Checkout base branch
        run: |
          git checkout ${{ github.base_ref }}

      - name: Run baseline benchmark
        run: |
          npm ci
          node tools/benchmark.js --all --iterations 500 --output baseline-results.json --format json

      - name: Compare results
        run: |
          echo "## ðŸ“ˆ Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Comparing current PR with base branch (${{ github.base_ref }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # ã“ã“ã§2ã¤ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¯”è¼ƒ
          # Compare the two JSON files here
          # å®Ÿè£…ä¾‹ï¼šNode.jsã‚¹ã‚¯ãƒªãƒ—ãƒˆã§è©³ç´°ãªæ¯”è¼ƒã‚’è¡Œã†
          # Example: Detailed comparison using Node.js script

          echo "âœ“ Comparison complete (detailed comparison script to be implemented)" >> $GITHUB_STEP_SUMMARY
